{
    "report_heading": "LLM-Augmented Model Selection and Advisory Report for {{dataset_name}}",
    "task": "You are a senior data scientist reviewing the results of an automated anomaly detection pipeline. Your task is to interpret symbolic scores, classifier metrics, dataset tags, and model knowledge to generate a structured and professional report. The AutoModelAdvisor integrates symbolic reasoning, empirical validation, and LLM guidance into a unified pipeline. Use the provided artifacts under report_structure: tables , heatmaps, and chart(s) to support your analysis. DO NOT generate your own hypothetical visuals or tables.Explain the model rankings in depth. Do not merely list metrics like Symbolic Score, ROC AUC, and F1 Minority. Instead, analyze what they mean, compare model trade-offs, highlight mismatches between symbolic and empirical results, and conclude which models perform better under different conditions.",
    "formatting_guidelines": {
        "heading_style": "Use Markdown heading level 3 (###) for all main section titles. Do not center-align headings. Ensure left alignment.",
        "alignment": "All section headings and content must be left-aligned.",
        "section_spacing": "Use consistent spacing between sections. Begin each section on a new line.",
        "visual_placement": "All visuals and image-based summaries (table, heatmaps, plots) must be placed only in the Annexure.",
        "table_generation": "Do NOT generate markdown or ASCII-style tables in the main body of the report.",
        "font_consistency": "Use consistent font size and style throughout the report. Avoid oversized headings or body text.",
        "exclude_from_output": "Do NOT reference or list any filenames from 'developer_notes_input_file_summary' (including json/docx/png files) in the report content or Annexure. These are internal developer inputs."
    },
    "executive_summary": {
        "instruction": "Provide a concise summary of the dataset, model evaluation objectives, key model recommendations, and advisory notes.",
        "sample": "Dataset used: {{dataset_name}}\nObjective: Automatically recommend the most suitable model(s) using symbolic reasoning and classifier metrics.\nEvaluation: Compared models on Symbolic Score, F1, ROC AUC, Accuracy, and AP.\nOutcome: [Top model(s) and justification]\nAdvisory Note: Suggested EDA or hyperparameters improvements if applicable."
    },
    "introduction": {
        "instruction": "Explain how the report is generated using symbolic reasoning, statistical profiling, and LLM augmentation.",
        "sample": "This report is generated using a hybrid model selection framework that integrates symbolic reasoning, statistical profiling, and Large Language Model (LLM) augmentation. The pipeline first analyzes the dataset's characteristics \u2014 such as dimensionality, size, and imbalance \u2014 and maps them to a set of predefined symbolic tags derived from domain knowledge and metadata. These tags are then used to rank candidate anomaly detection models using a symbolic scoring mechanism. In parallel, classifier performance metrics such as F1 Score, ROC AUC, Accuracy, and Average Precision are computed. A GPT-powered advisory module interprets these inputs to deliver expert-like recommendations."
    },
    "dataset_overview": {
        "instruction": "Using metadata and tags, write a short paragraph describing the dataset characteristics and implications: sample size, feature count, anomaly ratio, data quality, skewness, kurtosis, domain etc. After the paragraph, display the assigned tags exactly in the following format on a new line:\n\n**Assigned Tags**: {{tag_summary}}\n\nDo not rephrase or reorder the tags. Use the exact list from the dataset's JSON file.Reference the diagnostic EDA and basic EDA performed using the files listed. Explain the relevance of the assigned tags to the dataset characteristics. Use reference_files to enrich the content.",
        "metadata": {
            "sample_size": "<int>",
            "n_features": "<int>",
            "imbalance_ratio": "<float>",
            "domain": "<str>",
            "tag_summary": [
                "<tag1>",
                "<tag2>",
                "..."
            ]
        },
        "reference_files": [
            "outputs/llm_inputs/{{dataset_name}}_tags.json",
            "outputs/llm_inputs/EDA/{{dataset_name}}_eda_diagnostic.docx",
            "outputs/llm_inputs/basic_eda_steps.json"
        ]
    },
    "model_evaluation_summary_analysis": {
        "instruction": "Strictly Analyze (do not generate table) each model's performance based on the symbolic scores and empirical metrics provided in the reference files. DO NOT generate any tables yourself (markdown, text, or ASCII) in this section.. Refer to the Model Evaluation Summary (referenced as: 'ranking_table_img'), Model Evaluation Heatmap (referenced as: 'heatmap'), and Model-Wise Multi-Metric Rankings (referenced as: 'multi_metric'), and describe key insights from them. Do not use markdown formatting or generate any new table formatting in the output.Analyze how top models align across metrics and call out anomalies or inconsistencies. Use the following artifacts: ranking_table_img, heatmap and multi-metric.",
        "reference_files": {
            "ranking_table_img": "outputs/llm_inputs/{{dataset_name}}_llm_eval.png",
            "heatmap": "outputs/llm_inputs/heatmaps/{{dataset_name}}_rank_heatmap.png",
            "multi_metric": "outputs/llm_inputs/graphs/{{dataset_name}}_model_wise_multi_metric_rankings.png"
        }
    },
    "llm_recommendation_block": {
        "task": "Analyze symbolic ranks and evaluation metrics to recommend suitable models. Justify decisions based on symbolic tags, model tags and scores.",
        "comment": [
            "Use tag_summary(_tags.json), model_tag_knowledge.json and symbolic_score_formula.json for reasoning.",
            "Explain model suitability based on dataset characteristics and model characteristics.",
            "Flag unsuitable models with clear reasons.",
            "Justify selection and rejection decisions clearly.",
            "If classifier-best model is within top 3 symbolic ranks, consider it a valid recommendation and justify the usefulness of the symbolic rankings."
        ],
        "next_steps": [
            "Summarize top 1\u20132 models recommended.",
            "Present concise pros and cons of each."
        ],
        "report_structure": [
            "Header: LLM-Informed Recommendation and Justification",
            "Section A: Recommended Model(s) with Explanation",
            "Section B: Rejected/Unsuitable Models with Rationale"
        ]
    },
    "llm_advisory_block": {
        "task": "Analyze diagnostic EDA results and basic EDA steps and symbolic traits to recommend preprocessing and tuning strategies. Bridge empirical findings with symbolic reasoning for expert-like guidance.",
        "comment": [
            "Refer to EDA diagnostics and stage 2 cleaning steps.",
            "Refer to outputs/llm_inputs/EDA/{{dataset_name}}_eda_diagnostic.docx and outputs/llm_inputs/basic_eda_steps.json.",
            "Suggest preprocessing steps like normalization, encoding, outlier removal, dimensionality reduction as needed.",
            "Suggest model-specific and data context hyperparameter tuning."
        ],
        "next_steps": [
            "List concrete preprocessing actions.",
            "List hyperparameters to tune and rationale."
        ],
        "report_structure": [
            "Header: Data Preprocessing & Optimization Recommendations",
            "Section A: Additional EDA or Preprocessing Suggestions",
            "Section B: Hyperparameter Tuning Strategies for Recommended Models"
        ]
    },
    "llm_final_block": {
        "task": "Summarize the final model recommendation and outline steps for deployment readiness. Highlight how the system served both selection and advisory roles.",
        "comment": [
            "Recap top model(s) with rationale, emphasize readiness and advisory insights.",
            "Mention limitations and list 2\u20133 concrete next steps like cross-validation or production testing.",
            "Reaffirm model + advisory combo as the unique value proposition."
        ],
        "report_structure": [
            "Section A: Recommended Model(s) Summary",
            "Section B: Pre-Deployment Readiness Notes",
            "Section C: Recommended Actions to Prepare for Deployment"
        ]
    },
    "annexure_block": {
        "task": "Present only the visuals generated earlier, including: Model Evaluation Summary, Model Evaluation Heatmap , and Model-Wise Multi-Metric Rankings. These are placed at the end of the document as annexures and must NOT be re-analyzed or discussed in this section.",
        "comment": [
            "Do not repeat analysis here. This is only for displaying images/tables referred earlier.",
            "No new insights should be introduced in this section.",
            "Use proper titles like 'Model Evaluation Table', 'Model Evaluation Heatmap' and Model-Wise Multi-Metric Rankings."
        ],
        "report_structure": [
            "Section: Annexure",
            "{{summary_doc}}",
            "{{heatmap}}",
            "{{multi_metric}}"
        ]
    },
    "developer_notes_input_file_summary": {
        "note": "These files are used by the LLM for grounding but must NOT appear in the output report.",
        "static": [
            "basic_eda_steps.json",
            "tag_definitions.json",
            "tag_definitions_meta.json",
            "model_tag_knowledge.json",
            "model_tag_knowledge_meta.json",
            "tag_weights.json",
            "tag_weights_meta.json",
            "symbolic_score_formula.json"
        ],
        "dynamic": [
            "<dataset>_tags.json",
            "<dataset>_llm_input.csv",
            "<dataset>_llm_input_meta.json",
            "<dataset>_eda_diagnostic.docx",
            "<dataset>_llm_eval.docx",
            "<dataset>_llm_eval.png",
            "<dataset>_rank_heatmap.png",
            "<dataset>_ranking_bars_per_metric.png",
            "<dataset>_model_wise_multi_metric_rankings.png"
        ]
    },
    "report_structure": [
        "LLM-Augmented Model Selection and Advisory Report for {{dataset_name}}",
        "1. Executive Summary:",
        "2. Introduction: Methodology Behind the Recommendation",
        "3. Dataset Overview and Key Characteristics",
        "4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis",
        "5. Model Ranking Summary Analysis",
        "6. Visual Insights: Heatmap and Grouped Bar Plots Analysis",
        "7. LLM-Informed Recommendation and Justification",
        "8. Data Preprocessing & Optimization Recommendations ",
        "9. Hyperparameter Tuning and Guidance for Top Models",
        "10. Final Recommendation and Deployment Readiness",
        "11. Annexure"
    ]
}