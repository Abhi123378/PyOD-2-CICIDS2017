
    <style>
    h1, h2, h3, h4, h5, h6 {
        text-align: left !important;
    }
    </style>
    
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h2 { text-align: center; font-size: 20pt; }
            img { display: block; margin: 20px auto; max-width: 90%; }
        </style>
    </head>
    <body>
    <h3>LLM-Augmented Model Selection and Advisory Report for Optdigits</h3>

<h4>1. Executive Summary:</h4>

<p>This report evaluates the performance of various models on the Optdigits dataset, focusing on anomaly detection. The analysis integrates symbolic scores and empirical metrics to provide a comprehensive understanding of each model's strengths and weaknesses. The dataset's characteristics, such as high skewness and kurtosis, influence model performance, particularly in handling imbalanced data.</p>

<h4>2. Introduction: Methodology Behind the Recommendation</h4>

<p>The models were assessed using a combination of symbolic reasoning and empirical validation. Symbolic scores provide an initial ranking based on theoretical assumptions and model architecture suitability for the dataset's characteristics. Empirical metrics, including ROC AUC, Average Precision, and F1 score for the minority class, offer practical performance insights.</p>

<h4>3. Dataset Overview and Key Characteristics</h4>

<p>The Optdigits dataset is characterized by:
- Large sample size (5,216 samples)
- Medium dimensionality (64 features)
- Highly imbalanced with an anomaly ratio of 2.88%
- High skewness and kurtosis, with significant feature skewness and kurtosis percentages
- Clean data with no missing values</p>

<h4>4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis</h4>

<p>Symbolic scores provide a theoretical ranking, while empirical metrics reveal actual performance. Discrepancies between these can highlight areas where theoretical assumptions may not align with practical outcomes.</p>

<h4>5. Model Ranking Summary Analysis</h4>

<ul>
<li><strong>IForest</strong>: Symbolically ranked first, it also leads in empirical metrics with the highest ROC AUC (0.7178) and F1 Minority (0.0952). This model balances precision and recall effectively, making it suitable for highly imbalanced datasets.</li>
<li><strong>SO<em>GAAL</strong> and <strong>MO</em>GAAL</strong>: Both models share a symbolic rank of 2 but perform poorly empirically, with low ROC AUC and F1 scores. Despite high accuracy, their low recall indicates a struggle to identify anomalies effectively.</li>
<li><strong>LUNAR</strong>: Symbolically ranked third, it shows moderate empirical performance with a decent ROC AUC (0.508) and F1 Minority (0.0625). It offers a balance between precision and recall, suitable for datasets with moderate complexity.</li>
<li><strong>LOF</strong>: Although symbolically ranked fourth, it performs well empirically, especially in ROC AUC (0.5372) and Average Precision (0.0354). Its higher accuracy and precision suggest it handles structured data effectively.</li>
</ul>

<h4>6. Visual Insights: Heatmap and Grouped Bar Plots Analysis</h4>

<p>The heatmap and bar plots illustrate the trade-offs between models. IForest consistently outperforms others across metrics, while SO<em>GAAL and MO</em>GAAL lag significantly. LOF's performance is visually notable in precision and accuracy, indicating its robustness in structured environments.</p>

<h4>7. LLM-Informed Recommendation and Justification</h4>

<p>Based on the analysis, <strong>IForest</strong> is recommended for deployment due to its superior balance of precision, recall, and overall empirical performance. <strong>LOF</strong> is a viable alternative for environments prioritizing accuracy and precision.</p>

<h4>8. Data Preprocessing &amp; Optimization Recommendations</h4>

<p>To enhance model performance, consider:
- Addressing feature skewness and kurtosis through transformations
- Balancing the dataset using oversampling or synthetic data generation</p>

<h4>9. Hyperparameter Tuning and Guidance for Top Models</h4>

<p>For IForest, tuning parameters such as the number of estimators and maximum features can further optimize performance. LOF may benefit from adjusting the number of neighbors and contamination parameters.</p>

<h4>10. Final Recommendation and Deployment Readiness</h4>

<p>IForest is ready for deployment given its robust performance across metrics. LOF is a strong candidate for scenarios requiring high precision and accuracy.</p>

<h4>11. Annexure</h4>

<p>Detailed metrics and additional visualizations are provided for further analysis and reference.</p>

    <h2>Model Evaluation Table</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/optdigits_llm_eval.png" />
    <h2>Model Evaluation Heatmap</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/heatmaps/optdigits_rank_heatmap.png" />
    <h2>Model-wise Multi-Metric Rankings</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/graphs/optdigits_model_wise_multi_metric_rankings.png" />
    </body>
    </html>
    