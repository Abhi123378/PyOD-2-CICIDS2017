
    <style>
    h1, h2, h3, h4, h5, h6 {
        text-align: left !important;
    }
    </style>
    
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h2 { text-align: center; font-size: 20pt; }
            img { display: block; margin: 20px auto; max-width: 90%; }
        </style>
    </head>
    <body>
    <h3>LLM-Augmented Model Selection and Advisory Report for PIMA</h3>

<h4>1. Executive Summary:</h4>

<p>This report evaluates the performance of various anomaly detection models on the PIMA dataset, focusing on both symbolic and empirical metrics. The symbolic scores, derived from symbolic reasoning, are juxtaposed with empirical validation metrics such as ROC AUC, Average Precision, and F1 scores to provide a comprehensive analysis of model performance.</p>

<h4>2. Introduction: Methodology Behind the Recommendation</h4>

<p>The AutoModelAdvisor pipeline integrates symbolic reasoning with empirical validation and LLM guidance to rank models. Symbolic scores are computed based on theoretical model capabilities, while empirical metrics are derived from model performance on the dataset.</p>

<h4>3. Dataset Overview and Key Characteristics</h4>

<p>The PIMA dataset is characterized by:
- Medium sample size (768 samples)
- Low dimensionality (8 features)
- Balanced class distribution with an anomaly ratio of 34.9%
- Clean data with no missing values
- Moderate skewness and kurtosis, with some features exhibiting higher skewness and kurtosis.</p>

<h4>4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis</h4>

<p>All models received a symbolic score of 2.8, indicating a theoretical equivalence in potential performance. However, empirical metrics reveal significant differences:</p>

<ul>
<li><strong>LUNAR</strong>: Achieves the highest empirical performance with a ROC AUC of 0.6722 and an F1 (Minority) of 0.2551. This suggests strong discriminatory power and reasonable balance between precision and recall.</li>
<li><strong>AutoEncoder</strong>: While it shares the top symbolic rank, its empirical performance is slightly lower, with a ROC AUC of 0.6278 and an F1 (Minority) of 0.2145. This indicates a trade-off between symbolic potential and actual performance.</li>
<li><strong>VAE</strong>: Despite a lower ROC AUC of 0.5396, it ranks second in F1 (Minority), suggesting it may better capture minority class instances compared to AutoEncoder.</li>
<li><strong>LOF</strong>: Exhibits moderate empirical performance with a ROC AUC of 0.6014 but struggles with minority class detection, as indicated by its F1 (Minority) of 0.1502.</li>
<li><strong>DevNet</strong>: Shows the weakest empirical performance across all metrics, with a ROC AUC of 0.4061 and F1 (Minority) of 0.1333, indicating poor anomaly detection capability.</li>
</ul>

<h4>5. Model Ranking Summary Analysis</h4>

<p>The symbolic scores suggest theoretical equivalence, but empirical results highlight LUNAR as the superior model in practical terms. The discrepancy between symbolic and empirical rankings underscores the importance of empirical validation in model selection.</p>

<h4>6. Visual Insights: Heatmap and Grouped Bar Plots Analysis</h4>

<p>The heatmap and grouped bar plots (not provided here) would typically illustrate the comparative performance across models, highlighting the strengths of LUNAR in ROC AUC and F1 metrics.</p>

<h4>7. LLM-Informed Recommendation and Justification</h4>

<p>Based on empirical evidence, LUNAR is recommended for deployment due to its superior ROC AUC and F1 scores, indicating robust anomaly detection performance. The symbolic score alignment further supports its selection.</p>

<h4>8. Data Preprocessing &amp; Optimization Recommendations</h4>

<p>Given the dataset's characteristics, minimal preprocessing is required. However, addressing feature skewness and kurtosis through transformations could enhance model performance.</p>

<h4>9. Hyperparameter Tuning and Guidance for Top Models</h4>

<p>Fine-tuning LUNAR's hyperparameters, such as learning rate and regularization, could further optimize its performance. Exploring ensemble methods may also enhance robustness.</p>

<h4>10. Final Recommendation and Deployment Readiness</h4>

<p>LUNAR is deemed ready for deployment, contingent on further hyperparameter tuning and validation on a holdout set to ensure generalizability.</p>

<h4>11. Annexure</h4>

<p>Detailed metrics, model configurations, and additional insights are available in the annexure for further reference.</p>

    <h2>Model Evaluation Table</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/PIMA_llm_eval.png" />
    <h2>Model Evaluation Heatmap</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/heatmaps/PIMA_rank_heatmap.png" />
    <h2>Model-wise Multi-Metric Rankings</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/graphs/PIMA_model_wise_multi_metric_rankings.png" />
    </body>
    </html>
    