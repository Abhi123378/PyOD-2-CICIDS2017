
    <style>
    h1, h2, h3, h4, h5, h6 {
        text-align: left !important;
    }
    </style>
    
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h2 { text-align: center; font-size: 20pt; }
            img { display: block; margin: 20px auto; max-width: 90%; }
        </style>
    </head>
    <body>
    <h3>LLM-Augmented Model Selection and Advisory Report for Cardio Dataset</h3>

<h4>1. Executive Summary:</h4>

<p>This report evaluates anomaly detection models on the "cardio" dataset, characterized by medium sample size, medium dimensionality, and significant imbalance. The models are ranked based on symbolic scores and empirical performance metrics, offering insights into their strengths and weaknesses in detecting anomalies.</p>

<h4>2. Introduction: Methodology Behind the Recommendation</h4>

<p>The AutoModelAdvisor integrates symbolic reasoning with empirical validation to rank models. Symbolic scores are derived from model characteristics and theoretical performance, while empirical metrics like ROC AUC and F1 Minority provide real-world validation.</p>

<h4>3. Dataset Overview and Key Characteristics</h4>

<p>The dataset comprises 1,831 samples and 21 features, with an anomaly ratio of 9.6%. It is clean, with no missing values, but exhibits high skewness and kurtosis, impacting model performance.</p>

<h4>4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis</h4>

<ul>
<li><strong>IForest</strong>: Symbolically ranked first with a score of 4.2, it also leads in empirical metrics, achieving the highest ROC AUC (0.9205) and F1 Minority (0.5237). This alignment suggests robust performance in both theoretical and practical scenarios.</li>
<li><strong>MO<em>GAAL and SO</em>GAAL</strong>: Both models share a symbolic rank of 2 with a score of 3.1. MO<em>GAAL outperforms SO</em>GAAL empirically, with higher ROC AUC (0.7877 vs. 0.7794) and F1 Minority (0.468 vs. 0.3855). This indicates MO_GAAL's better adaptation to the dataset's characteristics despite similar symbolic scores.</li>
<li><strong>LUNAR</strong>: Also symbolically ranked 2, it underperforms empirically, with the lowest ROC AUC (0.6368) among the top-ranked models, highlighting a mismatch between symbolic expectations and empirical results.</li>
<li><strong>LOF</strong>: With a lower symbolic score of 2.0, LOF ranks last empirically, confirming its limited effectiveness on this dataset.</li>
</ul>

<h4>5. Model Ranking Summary Analysis</h4>

<p>The symbolic and empirical rankings generally align, with IForest consistently leading. However, the symbolic score's inability to distinguish between MO<em>GAAL, SO</em>GAAL, and LUNAR suggests limitations in capturing nuanced empirical performance differences.</p>

<h4>6. Visual Insights: Heatmap and Grouped Bar Plots Analysis</h4>

<p>The heatmap and bar plots (not shown here) would illustrate the stark contrast in performance metrics, particularly highlighting IForest's dominance and the close competition between MO<em>GAAL and SO</em>GAAL.</p>

<h4>7. LLM-Informed Recommendation and Justification</h4>

<p>Given the dataset's characteristics and the models' performance, IForest is recommended for deployment due to its superior empirical metrics and alignment with symbolic expectations. MO_GAAL is a viable alternative, offering a balance between precision and recall.</p>

<h4>8. Data Preprocessing &amp; Optimization Recommendations</h4>

<p>Addressing skewness and kurtosis through transformations could enhance model performance, particularly for models like LUNAR and LOF, which struggled with the dataset's distribution.</p>

<h4>9. Hyperparameter Tuning and Guidance for Top Models</h4>

<p>Further tuning of IForest's contamination parameter and MO_GAAL's network architecture could optimize their anomaly detection capabilities, especially in handling the dataset's imbalance.</p>

<h4>10. Final Recommendation and Deployment Readiness</h4>

<p>Deploy IForest for its robust performance across all metrics. MO_GAAL can be considered for scenarios requiring alternative model architectures or ensemble strategies.</p>

<h4>11. Annexure</h4>

<p>Detailed tables and charts supporting the analysis are available for further review.</p>

    <h2>Model Evaluation Table</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/cardio_llm_eval.png" />
    <h2>Model Evaluation Heatmap</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/heatmaps/cardio_rank_heatmap.png" />
    <h2>Model-wise Multi-Metric Rankings</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/graphs/cardio_model_wise_multi_metric_rankings.png" />
    </body>
    </html>
    