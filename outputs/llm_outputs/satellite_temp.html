
    <style>
    h1, h2, h3, h4, h5, h6 {
        text-align: left !important;
    }
    </style>
    
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h2 { text-align: center; font-size: 20pt; }
            img { display: block; margin: 20px auto; max-width: 90%; }
        </style>
    </head>
    <body>
    <h3>LLM-Augmented Model Selection and Advisory Report for Satellite Dataset</h3>

<h4>1. Executive Summary:</h4>

<p>This report evaluates anomaly detection models using both symbolic reasoning and empirical metrics. The models assessed include VAE, AutoEncoder, LOF, DeepSVDD, and DevNet. The analysis highlights the trade-offs between symbolic scores and empirical performance, providing insights into model suitability for the satellite dataset.</p>

<h4>2. Introduction: Methodology Behind the Recommendation</h4>

<p>The AutoModelAdvisor integrates symbolic reasoning with empirical validation to rank models. Symbolic scores consider theoretical model capabilities, while empirical metrics assess real-world performance on the dataset.</p>

<h4>3. Dataset Overview and Key Characteristics</h4>

<ul>
<li><strong>Dataset Tags</strong>: large<em>sample, medium</em>dimensional, balanced, clean<em>data, low</em>skewness, low<em>kurtosis, structured</em>data.</li>
<li><strong>Metrics</strong>: 6435 samples, 36 features, anomaly ratio of 31.64%, no missing values, low skewness and kurtosis.</li>
</ul>

<h4>4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis</h4>

<p>The symbolic scores for VAE, AutoEncoder, and LOF are identical at 4.6, indicating theoretical parity. However, empirical results show significant differences:</p>

<ul>
<li><strong>VAE</strong>: Highest ROC AUC (0.7427), precision (1.0), and accuracy (0.7837), but moderate recall (0.3163). This suggests VAE is highly precise but may miss some anomalies.</li>
<li><strong>AutoEncoder</strong>: Lower ROC AUC (0.6615) and precision (0.7112) than VAE, indicating a trade-off between precision and recall. It is less precise but captures a broader range of anomalies.</li>
<li><strong>LOF</strong>: Lowest empirical scores among the top three, with a ROC AUC of 0.5417. It struggles with precision and recall, making it less suitable despite a high symbolic score.</li>
</ul>

<h4>5. Model Ranking Summary Analysis</h4>

<ul>
<li><strong>VAE</strong>: Best overall empirical performance, leading in all ranks (F1, ROC AUC, AP, Accuracy). Ideal for scenarios prioritizing precision.</li>
<li><strong>AutoEncoder</strong>: Second in all empirical ranks, suitable for balanced precision-recall needs.</li>
<li><strong>LOF</strong>: Despite a high symbolic score, its empirical performance is subpar, indicating a mismatch between theoretical and practical capabilities.</li>
<li><strong>DeepSVDD and DevNet</strong>: Lower symbolic scores and empirical ranks, suggesting limited suitability for this dataset.</li>
</ul>

<h4>6. Visual Insights: Heatmap and Grouped Bar Plots Analysis</h4>

<p>The heatmap and bar plots (not provided here) would typically illustrate the stark contrast in empirical performance, particularly highlighting VAE's dominance in precision and accuracy.</p>

<h4>7. LLM-Informed Recommendation and Justification</h4>

<p>VAE is recommended for deployment due to its superior empirical performance, especially in precision. AutoEncoder is a viable alternative if a balance between precision and recall is desired.</p>

<h4>8. Data Preprocessing &amp; Optimization Recommendations</h4>

<p>Given the dataset's clean nature, no additional preprocessing is necessary. However, ensuring feature scaling and normalization could enhance model performance.</p>

<h4>9. Hyperparameter Tuning and Guidance for Top Models</h4>

<ul>
<li><strong>VAE</strong>: Focus on tuning latent space dimensions and learning rate.</li>
<li><strong>AutoEncoder</strong>: Optimize layer architecture and dropout rates to improve recall.</li>
</ul>

<h4>10. Final Recommendation and Deployment Readiness</h4>

<p>VAE is ready for deployment, offering the best trade-off between precision and recall for the satellite dataset's characteristics.</p>

<h4>11. Annexure</h4>

<p>Detailed empirical metrics and symbolic scores are available for further analysis and validation of the recommendations provided.</p>

    <h2>Model Evaluation Table</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/satellite_llm_eval.png" />
    <h2>Model Evaluation Heatmap</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/heatmaps/satellite_rank_heatmap.png" />
    <h2>Model-wise Multi-Metric Rankings</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/graphs/satellite_model_wise_multi_metric_rankings.png" />
    </body>
    </html>
    