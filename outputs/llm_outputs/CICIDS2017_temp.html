
    <style>
    h1, h2, h3, h4, h5, h6 {
        text-align: left !important;
    }
    </style>
    
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h2 { text-align: center; font-size: 20pt; }
            img { display: block; margin: 20px auto; max-width: 90%; }
        </style>
    </head>
    <body>
    <h3>LLM-Augmented Model Selection and Advisory Report for CICIDS2017</h3>

<h4>1. Executive Summary:</h4>

<p>This report evaluates the performance of various anomaly detection models on the CICIDS2017 dataset, which is characterized by large sample size, medium dimensionality, and significant imbalance. The models were ranked using both symbolic reasoning and empirical metrics, with the goal of identifying the most effective model for deployment.</p>

<h4>2. Introduction: Methodology Behind the Recommendation</h4>

<p>The AutoModelAdvisor integrates symbolic reasoning, empirical validation, and LLM guidance to rank models. Symbolic scores are derived from a combination of theoretical expectations and dataset characteristics, while empirical metrics like ROC AUC, F1 score, and precision-recall are used to validate these rankings.</p>

<h4>3. Dataset Overview and Key Characteristics</h4>

<p>The CICIDS2017 dataset is large (9783 samples) and medium-dimensional (69 features), with a significant imbalance (anomaly ratio of 18.5%). The dataset is clean, with no missing values, but exhibits high skewness and kurtosis, affecting 97.1% and 84.06% of features, respectively.</p>

<h4>4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis</h4>

<p>The symbolic scores and ranks provide an initial hypothesis about model performance based on dataset characteristics. However, empirical metrics offer a more nuanced view:</p>

<ul>
<li><p><strong>IForest</strong>: Symbolically ranked 1st with a score of 4.2, it also performs well empirically, leading in ROC AUC (0.6534) and ranking 2nd in F1 (Minority) and accuracy. This suggests a strong alignment between symbolic expectations and empirical results.</p></li>
<li><p><strong>SO_GAAL</strong>: Despite being symbolically ranked 2nd, it excels empirically, leading in F1 (Minority) and average precision, and achieving the highest accuracy. This indicates a potential underestimation in symbolic scoring, possibly due to its robustness to the dataset's skewness and kurtosis.</p></li>
<li><p><strong>MO_GAAL</strong> and <strong>LUNAR</strong>: Both models share a symbolic rank of 2 but perform poorly empirically, particularly in ROC AUC and F1 (Minority), highlighting a mismatch between symbolic expectations and empirical reality.</p></li>
<li><p><strong>LOF</strong>: With the lowest symbolic score and empirical performance, LOF's results are consistent across both evaluation methods.</p></li>
</ul>

<h4>5. Model Ranking Summary Analysis</h4>

<p>The symbolic and empirical analyses reveal trade-offs:</p>

<ul>
<li><strong>IForest</strong> is a balanced choice, performing consistently across metrics, making it suitable for general anomaly detection tasks.</li>
<li><strong>SO_GAAL</strong> offers superior precision and recall, making it ideal for scenarios where minimizing false negatives is critical.</li>
<li><strong>MO_GAAL</strong> and <strong>LUNAR</strong> may not be suitable for this dataset due to their lower empirical performance.</li>
<li><strong>LOF</strong> is the least effective, aligning with its symbolic rank.</li>
</ul>

<h4>6. Visual Insights: Heatmap and Grouped Bar Plots Analysis</h4>

<p>The heatmap and bar plots (not shown here) would illustrate the performance disparities across models, emphasizing the strengths of SO_GAAL in precision and recall compared to IForest's balanced performance.</p>

<h4>7. LLM-Informed Recommendation and Justification</h4>

<p>Based on the analysis, <strong>SO_GAAL</strong> is recommended for tasks prioritizing detection accuracy and precision, while <strong>IForest</strong> is suitable for balanced performance across various metrics.</p>

<h4>8. Data Preprocessing &amp; Optimization Recommendations</h4>

<p>Given the high skewness and kurtosis, preprocessing steps such as normalization or transformation could enhance model performance, particularly for models like MO_GAAL and LUNAR.</p>

<h4>9. Hyperparameter Tuning and Guidance for Top Models</h4>

<p>Further tuning of hyperparameters, especially for SO_GAAL and IForest, could optimize their performance. Focus on parameters affecting sensitivity to skewness and kurtosis.</p>

<h4>10. Final Recommendation and Deployment Readiness</h4>

<p><strong>SO_GAAL</strong> is recommended for deployment in environments where precision is paramount, while <strong>IForest</strong> is advised for general-purpose anomaly detection.</p>

<h4>11. Annexure</h4>

<p>Detailed tables, heatmaps, and additional charts supporting the analysis are included in the annexure section (not shown here).</p>

    <h2>Model Evaluation Table</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/CICIDS2017_llm_eval.png" />
    <h2>Model Evaluation Heatmap</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/heatmaps/CICIDS2017_rank_heatmap.png" />
    <h2>Model-wise Multi-Metric Rankings</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/graphs/CICIDS2017_model_wise_multi_metric_rankings.png" />
    </body>
    </html>
    