
    <style>
    h1, h2, h3, h4, h5, h6 {
        text-align: left !important;
    }
    </style>
    
    <html>
    <head>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            h2 { text-align: center; font-size: 20pt; }
            img { display: block; margin: 20px auto; max-width: 90%; }
        </style>
    </head>
    <body>
    <h3>LLM-Augmented Model Selection and Advisory Report for Lympho Dataset</h3>

<h4>1. Executive Summary:</h4>

<p>The objective of this report is to evaluate anomaly detection models for the lympho dataset, characterized by small sample size, medium dimensionality, and high imbalance. The report contrasts symbolic scores with empirical metrics to guide model selection.</p>

<h4>2. Introduction: Methodology Behind the Recommendation</h4>

<p>The AutoModelAdvisor integrates symbolic reasoning with empirical validation to rank models. Symbolic scores are derived from model characteristics and dataset compatibility, while empirical metrics assess actual performance.</p>

<h4>3. Dataset Overview and Key Characteristics</h4>

<ul>
<li><strong>Sample Size:</strong> 148</li>
<li><strong>Features:</strong> 18</li>
<li><strong>Anomaly Ratio:</strong> 4.05%</li>
<li><strong>Data Characteristics:</strong> No missing values, moderate skewness and kurtosis, indicating clean and structured data.</li>
</ul>

<h4>4. Symbolic Scoring vs. Empirical Evaluation: A Comparative Analysis</h4>

<p>Symbolic scores suggest IForest, SO<em>GAAL, and MO</em>GAAL as top models, each with a score of 2.4. However, empirical results reveal significant performance disparities.</p>

<h4>5. Model Ranking Summary Analysis</h4>

<ul>
<li><p><strong>IForest</strong>: Despite sharing the top symbolic rank, it outperforms others empirically with perfect ROC AUC and Average Precision scores. Its high recall (1.0) and moderate F1 score (0.5714) indicate strong anomaly detection but potential overfitting.</p></li>
<li><p><strong>SO<em>GAAL and MO</em>GAAL</strong>: Both models, while symbolically ranked first, show poor empirical performance with low ROC AUC and F1 scores. This suggests that symbolic scoring may not fully capture their limitations with highly imbalanced data.</p></li>
<li><p><strong>DeepSVDD</strong>: Ranked second symbolically, it shows strong empirical performance with high ROC AUC (0.973) and Average Precision (0.7302). Its balanced F1 score (0.4762) indicates effective anomaly detection with fewer false positives compared to IForest.</p></li>
<li><p><strong>LOF</strong>: Also symbolically ranked second, it performs well empirically with a ROC AUC of 0.9683. However, its lower precision (0.2667) compared to DeepSVDD suggests more false positives.</p></li>
</ul>

<h4>6. Visual Insights: Heatmap and Grouped Bar Plots Analysis</h4>

<p>The heatmap and bar plots (not shown here) would illustrate the stark contrast between symbolic and empirical rankings, highlighting IForest's dominance in empirical metrics and the underperformance of SO<em>GAAL and MO</em>GAAL.</p>

<h4>7. LLM-Informed Recommendation and Justification</h4>

<p>Given the dataset's characteristics and empirical results, IForest is recommended for its superior anomaly detection capabilities. DeepSVDD is a viable alternative, offering a balance between precision and recall.</p>

<h4>8. Data Preprocessing &amp; Optimization Recommendations</h4>

<ul>
<li><strong>Feature Scaling</strong>: Consider standardizing features to improve model performance.</li>
<li><strong>Imbalance Handling</strong>: Techniques like SMOTE could be explored to enhance minority class detection.</li>
</ul>

<h4>9. Hyperparameter Tuning and Guidance for Top Models</h4>

<ul>
<li><strong>IForest</strong>: Adjust the number of estimators and max samples to mitigate overfitting.</li>
<li><strong>DeepSVDD</strong>: Experiment with different kernel functions and hyperparameters to optimize detection.</li>
</ul>

<h4>10. Final Recommendation and Deployment Readiness</h4>

<p>IForest is ready for deployment, given its robust empirical performance. DeepSVDD can be considered for scenarios requiring balanced precision and recall.</p>

<h4>11. Annexure</h4>

<p>Includes detailed tables and charts for further analysis and validation of model performance.</p>

    <h2>Model Evaluation Table</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/lympho_llm_eval.png" />
    <h2>Model Evaluation Heatmap</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/heatmaps/lympho_rank_heatmap.png" />
    <h2>Model-wise Multi-Metric Rankings</h2>
    <img src="file:////home/exouser/Downloads/UofACP/outputs/llm_inputs/graphs/lympho_model_wise_multi_metric_rankings.png" />
    </body>
    </html>
    